
voc_model_id: 'ljspeech_raw'
tts_model_id: 'ljspeech_tts'

data_path: 'data/' # output data path

dsp:
  sample_rate: 22050
  n_fft: 1024
  num_mels: 80
  hop_length: 256                    # 12.5ms - in line with Tacotron 2 paper
  win_length: 1024                   # 50ms - same reason as above
  fmin: 0
  fmax: 8000

  peak_norm: False                   # Normalise to the peak of each wav file
  trim_start_end_silence: True      # Whether to trim leading and trailing silence
  trim_silence_top_db: 60            # Threshold in decibels below reference to consider silence for for trimming
                                      # start and end silences with librosa (no trimming if really high)
  pitch_max_freq: 600                # Maximum value for pitch frequency to remove outliers (Common pitch range is
                                      # about 60-300)
  trim_long_silences: False            # Whether to reduce long silence using WebRTC Voice Activity Detector
  vad_window_length: 30      # In milliseconds
  vad_moving_average_width: 8
  vad_max_silence_length: 12
  vad_sample_rate: 16000

  # vocoder
  voc_mode: 'RAW'                   # RAW or MOL
  bits: 9                            # bit depth of signal
  mu_law: True                       # Recommended to suppress noise if using raw bits in hp.voc_mode below


preprocessing:
  seed: 42
  n_val: 200
  language: 'en-us'
  cleaner_name: 'english_cleaners'
  min_text_len: 2

  # Duration Extraction from Attention
  extract_durations_with_dijkstra: True    # slower but much more robust than simply counting attention peaks


vocoder:
  model:
    mode: 'RAW'                    # either 'RAW' (softmax on raw bits) or 'MOL' (sample from mixture of logistics)
    upsample_factors: [4, 8, 8]   # NB - this needs to correctly factorise hop_length
    rnn_dims: 512
    fc_dims: 512
    compute_dims: 128
    res_out_dims: 128
    res_blocks: 10
    pad: 2                         # this will pad the input so that the resnet can 'see' wider than input length


  training:
    schedule:
      - 1e-4,  300_000,  32   # progressive training schedule
      - 1e-5,  600_000,  32   # lr, step, batch_size

    checkpoint_every: 25_000
    gen_samples_every: 5000        # how often to generate samples for cherry-picking models
    num_gen_samples: 3             # number of samples to generate for cherry-picking models
    keep_top_k: 3                  # how many top performing models to keep
    seq_len: 1280            # must be a multiple of hop_length
    clip_grad_norm: 4              # set to None if no gradient clipping needed
    max_mel_len: 20000

    # Generating / Synthesizing
    gen_batched: True              # very fast (realtime+) single utterance batched generation
    target: 11_000                 # target number of samples to be generated in each batch entry
    overlap: 550                   # number of samples for crossfading between batches


tacotron:
  model:
    embed_dims: 256                # embedding dimension for the graphemes/phoneme inputs
    encoder_dims: 128
    decoder_dims: 256
    postnet_dims: 128
    encoder_K: 16
    lstm_dims: 512
    postnet_K: 8
    num_highways: 4
    dropout: 0.5
    stop_threshold: -11           # Value below which audio generation ends.

  training:
    schedule:
      - 10,  1e-3,  10_000,  32   # progressive training schedule
      - 5,   1e-4,  20_000,  16   # (r, lr, step, batch_size)
      - 2,   1e-4,  30_000,  8
      - 1,   1e-4,  50_000,  8

    max_mel_len: 1250              # if you have a couple of extremely long spectrograms you might want to use this
    clip_grad_norm: 1.0            # clips the gradient norm to prevent explosion - set to None if not needed
    checkpoint_every: 10_000       # checkpoints the model every X steps
    plot_every: 1000


forward_tacotron:
  model:
    embed_dims: 256             # embedding dimension for the graphemes/phoneme inputs
    prenet_dims: 256
    postnet_dims: 256
    durpred_conv_dims: 256
    durpred_rnn_dims: 64
    durpred_dropout: 0.5

    pitch_conv_dims: 256
    pitch_rnn_dims: 128
    pitch_dropout: 0.5
    pitch_emb_dims: 64           # embedding dimension of pitch, set to 0 if you don't want pitch conditioning
    pitch_proj_dropout: 0.

    prenet_K: 16
    postnet_K: 8
    rnn_dims: 512
    num_highways: 4
    dropout: 0.1

  training:
    schedule:
      - 1e-3,  150_000,  32   # progressive training schedule
      - 1e-4,  300_000,  32   # lr, step, batch_size

    max_mel_len: 1250              # if you have a couple of extremely long spectrograms you might want to use this
    clip_grad_norm: 1.0            # clips the gradient norm to prevent explosion - set to None if not needed
    checkpoint_every: 10_000        # checkpoints the model every X steps
    plot_every: 1000

    filter_attention: True               # whether to filter data with bad attention scores
    min_attention_sharpness: 0.5         # filter data with bad attention sharpness score, if 0 then no filter
    min_attention_alignment: 0.95        # filter data with bad attention alignment score, if 0 then no filter